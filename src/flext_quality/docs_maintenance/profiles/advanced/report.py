#!/usr/bin/env python3
"""Documentation Quality Assurance Reporting System.

Generates complete reports, dashboards, and analytics for documentation maintenance.
Provides visualization and tracking of quality metrics over time.
"""

import argparse
import json
import logging
import shutil
from collections.abc import Mapping
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import TypedDict, cast

import yaml

from flext_quality.docs_maintenance.utils import (
    get_maintenance_dir,
    get_project_root,
    get_reports_dir,
)

from .audit import DocumentationAuditor
from .validate_links import LinkValidator
from .validate_style import StyleValidator

logger = logging.getLogger(__name__)

# Constants for report generation
MIN_HISTORICAL_REPORTS_FOR_TRENDS: int = 2
MIN_SCORES_FOR_TREND_ANALYSIS: int = 3
MAX_BROKEN_LINKS_THRESHOLD: int = 10
MIN_STYLE_SCORE_THRESHOLD: int = 80


# Type definitions
class AuditSummary(TypedDict):
    """Summary of audit results."""

    total_files: int
    total_words: int
    average_quality: float
    critical_issues: int
    files_audited: int


class ValidationSummary(TypedDict):
    """Summary of validation results."""

    broken_links: int
    external_links: int
    internal_links: int
    files_checked: int
    total_links: int


class StyleSummary(TypedDict):
    """Summary of style validation results."""

    total_violations: int
    average_score: float
    files_with_violations: int


class TrendData(TypedDict):
    """Historical trend data."""

    available: bool
    recent_scores: list[float]
    direction: str
    message: str


class RecommendationInfo(TypedDict):
    """Information about a recommendation."""

    title: str
    priority: str
    description: str
    actions: list[str]


class ReportConfig(TypedDict):
    """Configuration for report generation."""

    reporting: dict[str, object]


DEFAULT_AUDIT_SUMMARY = cast(
    "AuditSummary",
    {
        "total_files": 0,
        "total_words": 0,
        "average_quality": 0.0,
        "critical_issues": 0,
        "files_audited": 0,
    },
)

DEFAULT_VALIDATION_SUMMARY = cast(
    "ValidationSummary",
    {
        "broken_links": 0,
        "external_links": 0,
        "internal_links": 0,
        "files_checked": 0,
        "total_links": 0,
    },
)

DEFAULT_STYLE_SUMMARY = cast(
    "StyleSummary",
    {
        "total_violations": 0,
        "average_score": 100.0,
        "files_with_violations": 0,
    },
)


@dataclass
class ReportData:
    """Container for all report data."""

    timestamp: datetime
    audit_summary: AuditSummary
    validation_summary: ValidationSummary
    style_summary: StyleSummary
    trends: TrendData
    recommendations: list[RecommendationInfo]


@dataclass
class QualityMetrics:
    """Quality metrics for documentation health."""

    overall_score: float
    content_health: float
    link_health: float
    style_consistency: float
    accessibility: float
    trends_direction: str  # 'improving', 'stable', 'declining'


class ReportGenerator:
    """Main report generation class."""

    def __init__(self, config_path: str | None = None) -> None:
        """Initialize report generator with optional configuration file."""
        self.config: ReportConfig = self._load_config(config_path)
        self.project_root = get_project_root()
        self.maintenance_dir = get_maintenance_dir(self.project_root)
        self.reports_dir = str(get_reports_dir(self.project_root))

    def _load_config(self, config_path: str | None = None) -> ReportConfig:
        """Load configuration."""
        default_config = {
            "reporting": {
                "output_formats": ["markdown"],
                "include_charts": True,
                "chart_style": "seaborn",
                "metrics_history_days": 30,
                "dashboard_template": "default",
            },
            "thresholds": {
                "excellent_score": 90,
                "good_score": 70,
                "fair_score": 50,
                "critical_issues_threshold": 10,
            },
        }

        if config_path and Path(config_path).exists():
            with Path(config_path).open(encoding="utf-8") as f:
                user_config = yaml.safe_load(f)
                for key, value in user_config.items():
                    if key in default_config:
                        default_config[key].update(value)
                    else:
                        default_config[key] = value

        return cast("ReportConfig", default_config)

    def generate_comprehensive_report(
        self,
        audit_file: str | None = None,
        validation_file: str | None = None,
        style_file: str | None = None,
    ) -> ReportData:
        """Generate complete report from audit results."""
        # Load data from files or run fresh audits
        audit_data = self._load_audit_data(audit_file)
        validation_data = self._load_validation_data(validation_file)
        style_data = self._load_style_data(style_file)

        audit_summary = cast(
            "AuditSummary",
            self._merge_summary(audit_data, DEFAULT_AUDIT_SUMMARY),
        )
        validation_summary = cast(
            "ValidationSummary",
            self._merge_summary(validation_data, DEFAULT_VALIDATION_SUMMARY),
        )
        validation_summary["total_links"] = (
            validation_summary["broken_links"]
            + validation_summary["external_links"]
            + validation_summary["internal_links"]
        )
        style_summary = cast(
            "StyleSummary",
            self._merge_summary(style_data, DEFAULT_STYLE_SUMMARY),
        )

        # Calculate trends
        trends = self._calculate_trends()

        # Generate recommendations
        recommendations = self._generate_recommendations(
            audit_summary,
            validation_summary,
            style_summary,
        )

        return ReportData(
            timestamp=datetime.now(UTC),
            audit_summary=audit_summary,
            validation_summary=validation_summary,
            style_summary=style_summary,
            trends=trends,
            recommendations=recommendations,
        )

    def _merge_summary(
        self,
        data: Mapping[str, object],
        defaults: Mapping[str, object],
    ) -> dict[str, object]:
        """Ensure summary dictionaries contain required keys."""
        merged = dict(defaults)
        summary_candidate: dict[str, object] | None = None

        if isinstance(data, dict):
            candidate = data.get("summary")
            if isinstance(candidate, dict):
                summary_candidate = candidate
            else:
                summary_candidate = {
                    key: data.get(key) for key in defaults if key in data
                }

        if summary_candidate:
            merged.update({k: v for k, v in summary_candidate.items() if v is not None})

        return merged

    def _load_audit_data(self, audit_file: str | None) -> AuditSummary:
        """Load audit data."""
        if audit_file and Path(audit_file).exists():
            with Path(audit_file).open(encoding="utf-8") as f:
                return json.load(f)
        # For now, return a default audit summary
        return AuditSummary(
            total_files=0,
            total_words=0,
            average_quality=0.0,
            critical_issues=0,
            files_audited=0,
        )

    def _load_validation_data(self, validation_file: str | None) -> ValidationSummary:
        """Load validation data."""
        if validation_file and Path(validation_file).exists():
            with Path(validation_file).open(encoding="utf-8") as f:
                return json.load(f)
        # For now, return a default validation summary
        return ValidationSummary(
            broken_links=0,
            external_links=0,
            internal_links=0,
            files_checked=0,
            total_links=0,
        )

    def _load_style_data(self, style_file: str | None) -> StyleSummary:
        """Load style data."""
        if style_file and Path(style_file).exists():
            with Path(style_file).open(encoding="utf-8") as f:
                return json.load(f)
        # For now, return a default style summary
        return StyleSummary(
            total_violations=0,
            average_score=100.0,
            files_with_violations=0,
        )

    def _run_quick_audit(
        self,
    ) -> dict[str, object]:  # Keep for compatibility, will refactor later
        """Run a quick audit for basic metrics."""
        # Import here to avoid circular imports

        auditor = DocumentationAuditor()
        results = auditor.audit_directory(
            str(self.project_root / "docs"),
            recursive=False,
        )
        summary = auditor.generate_summary()

        return {
            "summary": asdict(summary),
            "results": [asdict(r) for r in results[:10]],  # Limit for quick audit
        }

    def _run_quick_validation(self) -> dict[str, object]:  # Keep for compatibility
        """Run quick link validation."""
        validator = LinkValidator()
        results = validator.validate_directory(
            str(self.project_root / "docs"),
            check_external=False,  # Quick mode
        )
        summary = validator.generate_summary(results)

        return {"summary": asdict(summary), "results": [asdict(r) for r in results[:5]]}

    def _run_quick_style_check(self) -> dict[str, object]:  # Keep for compatibility
        """Run quick style validation."""
        validator = StyleValidator()
        results = validator.validate_directory(str(self.project_root / "docs"))
        summary = validator.generate_summary(results)

        return {"summary": asdict(summary), "results": [asdict(r) for r in results[:5]]}

    def _calculate_trends(self) -> TrendData:
        """Calculate quality trends from historical data."""
        # Look for historical reports
        history_dir = str(Path(self.reports_dir) / "history")
        if not Path(history_dir).exists():
            return TrendData(
                available=False,
                recent_scores=[],
                direction="stable",
                message="No historical data available for trend analysis",
            )

        # Load recent reports
        recent_reports = []
        for file in sorted(Path(history_dir).iterdir())[-7:]:  # Last 7 reports
            if file.is_file() and file.suffix == ".json":
                try:
                    with file.open(encoding="utf-8") as f:
                        report = json.load(f)
                        recent_reports.append(report)
                except Exception as e:
                    # Skip invalid report files silently - they may be corrupted or incomplete
                    logger.debug(f"Skipping invalid report file {file.name}: {e}")
                    continue

        if len(recent_reports) < MIN_HISTORICAL_REPORTS_FOR_TRENDS:
            return TrendData(
                available=False,
                recent_scores=[],
                direction="stable",
                message=f"Need at least 2 reports for trends, found {len(recent_reports)}",
            )

        # Calculate trends
        scores = [
            r.get("quality_metrics", {}).get("overall_score", 0) for r in recent_reports
        ]
        trend_direction = "stable"

        if len(scores) >= MIN_SCORES_FOR_TREND_ANALYSIS:
            recent_avg = (
                sum(scores[-MIN_SCORES_FOR_TREND_ANALYSIS:])
                / MIN_SCORES_FOR_TREND_ANALYSIS
            )
            older_avg = sum(scores[:-3]) / max(1, len(scores[:-3]))

            if recent_avg > older_avg + 5:
                trend_direction = "improving"
            elif recent_avg < older_avg - 5:
                trend_direction = "declining"

        return TrendData(
            available=True,
            recent_scores=scores[-5:],
            direction=trend_direction,
            message=f"Analyzed {len(recent_reports)} reports",
        )

    def _generate_recommendations(
        self,
        audit_data: AuditSummary,
        validation_data: ValidationSummary,
        style_data: StyleSummary,
    ) -> list[RecommendationInfo]:
        """Generate actionable recommendations."""
        recommendations = []

        # Audit-based recommendations
        if audit_data["critical_issues"] > 0:
            recommendations.append(
                RecommendationInfo(
                    title="Address Critical Content Issues",
                    priority="high",
                    description=f"{audit_data['critical_issues']} critical content issues require immediate attention",
                    actions=[
                        "Review files with quality score < 50",
                        "Update outdated content (>90 days old)",
                        "Fix broken internal references",
                    ],
                ),
            )

        # Link validation recommendations
        if validation_data["broken_links"] > MAX_BROKEN_LINKS_THRESHOLD:
            recommendations.append(
                RecommendationInfo(
                    title="Fix Broken Links",
                    priority="medium",
                    description=f"{validation_data['broken_links']} broken links detected across documentation",
                    actions=[
                        "Update or remove broken external links",
                        "Fix incorrect internal references",
                        "Review most broken domains",
                    ],
                ),
            )

        # Style recommendations
        if style_data["average_score"] < MIN_STYLE_SCORE_THRESHOLD:
            recommendations.append(
                RecommendationInfo(
                    title="Improve Style Consistency",
                    priority="low",
                    description=f"Average style score of {style_data['average_score']:.1f}/100 indicates formatting issues",
                    actions=[
                        "Fix heading hierarchy violations",
                        "Add language specifications to code blocks",
                        "Remove trailing whitespace",
                    ],
                ),
            )

        # Default recommendations
        if not recommendations:
            recommendations.append(
                RecommendationInfo(
                    title="Schedule Regular Maintenance",
                    priority="info",
                    description="Documentation quality is good, continue regular maintenance",
                    actions=[
                        "Run weekly complete audits",
                        "Monitor link health monthly",
                        "Review content freshness quarterly",
                    ],
                ),
            )

        return recommendations

    def _calculate_quality_metrics(self, report_data: ReportData) -> QualityMetrics:
        """Calculate overall quality metrics from report data."""
        # Calculate content health (weighted average of audit quality)
        content_health = (
            report_data.audit_summary["average_quality"]
            if report_data.audit_summary["total_files"] > 0
            else 50.0
        )

        # Calculate link health (inverse of broken links ratio)
        total_links = (
            report_data.validation_summary["broken_links"]
            + report_data.validation_summary["external_links"]
            + report_data.validation_summary["internal_links"]
        )
        link_health = (
            100.0
            * (
                1
                - (report_data.validation_summary["broken_links"] / max(total_links, 1))
            )
            if total_links > 0
            else 100.0
        )

        # Calculate style consistency (inverse of violations ratio)
        style_consistency = (
            100.0
            * (
                1
                - (
                    report_data.style_summary["total_violations"]
                    / max(report_data.style_summary["files_with_violations"], 1)
                )
            )
            if report_data.style_summary["files_with_violations"] > 0
            else 100.0
        )

        # Calculate accessibility (estimated based on content health and link health)
        accessibility = (content_health + link_health) / 2

        # Calculate overall score (weighted average)
        overall_score = (
            content_health * 0.4
            + link_health * 0.3
            + style_consistency * 0.2
            + accessibility * 0.1
        )

        # Determine trends direction (simplified - would need historical data)
        trends_direction = "stable"

        return QualityMetrics(
            overall_score=overall_score,
            content_health=content_health,
            link_health=link_health,
            style_consistency=style_consistency,
            accessibility=accessibility,
            trends_direction=trends_direction,
        )

    def generate_dashboard(
        self,
        report_data: ReportData,
        output_file: str = "dashboard.html",
    ) -> str:
        """Generate HTML dashboard (requires visualization libraries)."""
        template = self._get_dashboard_template()

        # Calculate quality metrics
        metrics = self._calculate_quality_metrics(report_data)

        # Prepare data for template
        template_data = {
            "timestamp": report_data.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            "overall_score": metrics.overall_score,
            "content_health": metrics.content_health,
            "link_health": metrics.link_health,
            "style_consistency": metrics.style_consistency,
            "accessibility": metrics.accessibility,
            "trends_direction": metrics.trends_direction,
            "audit_summary": report_data.audit_summary,
            "validation_summary": report_data.validation_summary,
            "style_summary": report_data.style_summary,
            "recommendations": report_data.recommendations,
            "trends": report_data.trends,
        }

        # Render template
        dashboard_html = template.format(**template_data)

        Path(self.reports_dir).mkdir(exist_ok=True, parents=True)
        output_path = str(Path(self.reports_dir) / output_file)
        with Path(output_path).open("w", encoding="utf-8") as f:
            f.write(dashboard_html)

        return output_path

    def write_json_report(
        self,
        report_data: ReportData,
        output_file: str = "report.json",
    ) -> str:
        """Persist report data as JSON."""
        Path(self.reports_dir).mkdir(exist_ok=True, parents=True)
        metrics = self.calculate_quality_metrics(report_data)

        payload = {
            "timestamp": report_data.timestamp.isoformat(),
            "audit_summary": report_data.audit_summary,
            "validation_summary": report_data.validation_summary,
            "style_summary": report_data.style_summary,
            "trends": report_data.trends,
            "recommendations": report_data.recommendations,
            "quality_metrics": asdict(metrics),
        }

        output_path = str(Path(self.reports_dir) / output_file)
        with Path(output_path).open("w", encoding="utf-8") as f:
            json.dump(payload, f, indent=2)

        return output_path

    def generate_markdown_report(
        self,
        report_data: ReportData,
        output_file: str = "report.md",
    ) -> str:
        """Generate a markdown summary for the provided report data."""
        Path(self.reports_dir).mkdir(exist_ok=True, parents=True)
        metrics = self._calculate_quality_metrics(report_data)

        summary = f"""# ðŸ“Š Documentation Quality Summary

**Generated:** {report_data.timestamp.strftime("%Y-%m-%d %H:%M:%S")}

## ðŸŽ¯ Overall Quality Score: {metrics.overall_score:.1f}/100

### Quality Breakdown
- **Content Health:** {metrics.content_health:.1f}/100
- **Link Health:** {metrics.link_health:.1f}/100
- **Style Consistency:** {metrics.style_consistency:.1f}/100
- **Accessibility:** {metrics.accessibility:.1f}/100

### Trends
- **Direction:** {metrics.trends_direction.title()}
- **Recent Scores:** {", ".join(map(str, report_data.trends.get("recent_scores", [])))}

## ðŸ“ˆ Key Metrics

### Content Quality
- **Files Audited:** {report_data.audit_summary.get("total_files", 0)}
- **Total Words:** {report_data.audit_summary.get("total_words", 0):,}
- **Average Age:** {report_data.audit_summary.get("average_age", 0):.1f} days
- **Critical Issues:** {report_data.audit_summary.get("critical_issues", 0)}

### Link Health
- **Total Links:** {report_data.validation_summary.get("total_links", 0)}
- **Broken Links:** {report_data.validation_summary.get("broken_links", 0)}
- **External Links:** {report_data.validation_summary.get("external_links", 0)}
- **Internal Links:** {report_data.validation_summary.get("internal_links", 0)}

### Style Consistency
- **Files Checked:** {report_data.style_summary.get("files_checked", report_data.style_summary.get("total_files", 0))}
- **Total Violations:** {report_data.style_summary.get("total_violations", 0)}
- **Average Score:** {report_data.style_summary.get("average_score", 0):.1f}/100
- **Files with Issues:** {report_data.style_summary.get("files_with_violations", 0)}

## ðŸŽ¯ Priority Actions

"""

        for rec in report_data.recommendations:
            summary += (
                f"### {rec['title']} ({rec['priority'].title()} Priority)\n"
                f"{rec['description']}\n\n"
                "**Actions:**\n"
                + "".join(f"- {action}\n" for action in rec["actions"])
                + "\n"
            )

        output_path = str(Path(self.reports_dir) / output_file)
        with Path(output_path).open("w", encoding="utf-8") as f:
            f.write(summary)

        return output_path

    def calculate_quality_metrics(self, report_data: ReportData) -> QualityMetrics:
        """Calculate overall quality metrics."""
        audit_score = float(report_data.audit_summary["average_quality"])
        link_broken = int(report_data.validation_summary["broken_links"])
        link_total = int(report_data.validation_summary["total_links"])
        style_score = float(report_data.style_summary["average_score"])

        # Content health (60% weight on audit)
        content_health = audit_score * 0.6

        # Link health (inverse of broken link ratio)
        link_health: float = (
            max(0.0, 100.0 - (link_broken / link_total * 100.0))
            if link_total > 0
            else 100.0
        )

        # Style consistency (40% weight on style)
        style_consistency = style_score * 0.4

        # Accessibility (estimated from style and links)
        accessibility = (style_score + link_health) / 2

        # Overall score
        overall_score = (
            content_health * 0.4
            + link_health * 0.3
            + style_consistency * 0.2
            + accessibility * 0.1
        )

        # Trends direction
        trends_direction = report_data.trends.get("direction", "stable")

        return QualityMetrics(
            overall_score=round(overall_score, 1),
            content_health=round(content_health, 1),
            link_health=round(link_health, 1),
            style_consistency=round(style_consistency, 1),
            accessibility=round(accessibility, 1),
            trends_direction=trends_direction,
        )

    def export_report(
        self,
        report_data: ReportData,
        formats: list[str] | None = None,
    ) -> dict[str, list[str]]:
        """Generate report outputs for the requested formats."""
        raw_formats = formats or self.config["reporting"].get(
            "output_formats",
            ["markdown"],
        )
        # Ensure we have a proper list of strings
        if isinstance(raw_formats, list):
            requested_formats: list[str] = [str(f) for f in raw_formats]
        else:
            requested_formats = ["markdown"]

        timestamp_slug = report_data.timestamp.strftime("%Y%m%d_%H%M%S")
        generated: dict[str, list[str]] = {}

        for fmt in requested_formats:
            try:
                if fmt == "markdown":
                    path = self.generate_markdown_report(
                        report_data,
                        output_file=f"report_{timestamp_slug}.md",
                    )
                elif fmt == "json":
                    path = self.write_json_report(
                        report_data,
                        output_file=f"report_{timestamp_slug}.json",
                    )
                elif fmt == "html":
                    path = self.generate_dashboard(
                        report_data,
                        output_file=f"dashboard_{timestamp_slug}.html",
                    )
                else:
                    logger.warning("Unsupported report format requested: %s", fmt)
                    continue

                generated.setdefault(fmt, []).append(path)

                # Maintain latest pointers for convenience
                if fmt in {"markdown", "json"}:
                    latest_name = (
                        "latest_report.md"
                        if fmt == "markdown"
                        else "latest_report.json"
                    )
                    latest_path = Path(self.reports_dir) / latest_name
                    shutil.copyfile(path, latest_path)
                elif fmt == "html":
                    latest_path = Path(self.reports_dir) / "latest_dashboard.html"
                    shutil.copyfile(path, latest_path)

            except Exception:  # pragma: no cover - defensive logging
                logger.exception("Failed to generate %s report", fmt)

        return generated

    def _get_dashboard_template(self) -> str:
        """Get HTML dashboard template."""
        return """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation Quality Dashboard</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .score {{
            font-size: 48px;
            font-weight: bold;
            margin: 10px 0;
        }}
        .status {{
            font-size: 18px;
            opacity: 0.9;
        }}
        .metrics {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
        }}
        .metric {{
            text-align: center;
            padding: 20px;
            border-radius: 8px;
            background: #f8f9fa;
        }}
        .metric-value {{
            font-size: 32px;
            font-weight: bold;
            color: #495057;
        }}
        .metric-label {{
            font-size: 14px;
            color: #6c757d;
            margin-top: 5px;
        }}
        .recommendations {{
            padding: 30px;
            border-top: 1px solid #e9ecef;
        }}
        .recommendation {{
            margin-bottom: 20px;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid;
        }}
        .priority-high {{ border-color: #dc3545; background: #f8d7da; }}
        .priority-medium {{ border-color: #ffc107; background: #fff3cd; }}
        .priority-low {{ border-color: #28a745; background: #d4edda; }}
        .priority-info {{ border-color: #17a2b8; background: #d1ecf1; }}
        .timestamp {{
            text-align: center;
            color: #6c757d;
            font-size: 14px;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ðŸ“Š Documentation Quality Dashboard</h1>
            <div class="score">{overall_score}/100</div>
            <div class="status">Overall Quality Score</div>
            <div class="timestamp">Generated: {timestamp}</div>
        </div>

        <div class="metrics">
            <div class="metric">
                <div class="metric-value">{content_health}</div>
                <div class="metric-label">Content Health</div>
            </div>
            <div class="metric">
                <div class="metric-value">{link_health}</div>
                <div class="metric-label">Link Health</div>
            </div>
            <div class="metric">
                <div class="metric-value">{style_consistency}</div>
                <div class="metric-label">Style Consistency</div>
            </div>
            <div class="metric">
                <div class="metric-value">{accessibility}</div>
                <div class="metric-label">Accessibility</div>
            </div>
        </div>

        <div class="recommendations">
            <h2>ðŸŽ¯ Recommendations</h2>
            {"".join([f'''
            <div class="recommendation priority-{rec["priority"]}">
                <h4>{rec["title"]}</h4>
                <p>{rec["description"]}</p>
                <ul>
                    {"".join([f"<li>{action}</li>" for action in rec["actions"]])}
                </ul>
            </div>
            ''' for rec in recommendations])}
        </div>
    </div>
</body>
</html>"""

    def generate_weekly_summary(
        self,
        report_data: ReportData,
        output_file: str = "weekly-summary.md",
    ) -> str:
        """Backward-compatible wrapper for markdown report generation."""
        return self.generate_markdown_report(report_data, output_file=output_file)


def main() -> None:
    """Main entry point for documentation reporting system."""
    parser = argparse.ArgumentParser(
        description="Documentation Quality Assurance Reporting System",
    )
    parser.add_argument("--audit-file", help="Path to audit results JSON file")
    parser.add_argument(
        "--validation-file",
        help="Path to validation results JSON file",
    )
    parser.add_argument("--style-file", help="Path to style results JSON file")
    parser.add_argument(
        "--generate-dashboard",
        action="store_true",
        help="Generate HTML dashboard",
    )
    parser.add_argument(
        "--weekly-summary",
        action="store_true",
        help="Generate weekly markdown summary",
    )
    parser.add_argument(
        "--monthly-report",
        action="store_true",
        help="Generate detailed monthly report",
    )
    parser.add_argument(
        "--output-dir",
        default="reports",
        help="Output directory for reports",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")

    args = parser.parse_args()

    generator = ReportGenerator()
    generator.reports_dir = args.output_dir
    Path(generator.reports_dir).mkdir(exist_ok=True, parents=True)

    if args.verbose:
        pass

    # Generate complete report
    report_data = generator.generate_comprehensive_report(
        args.audit_file,
        args.validation_file,
        args.style_file,
    )

    generated_files = []

    if args.generate_dashboard:
        dashboard_file = generator.generate_dashboard(report_data)
        generated_files.append(("Dashboard", dashboard_file))

    if args.weekly_summary:
        summary_file = generator.generate_weekly_summary(report_data)
        generated_files.append(("Weekly Summary", summary_file))

    if args.monthly_report:
        # Monthly report would be more detailed
        monthly_file = generator.generate_dashboard(report_data, "monthly-report.html")
        generated_files.append(("Monthly Report", monthly_file))

    # If no specific report requested, generate dashboard
    if not any([args.generate_dashboard, args.weekly_summary, args.monthly_report]):
        dashboard_file = generator.generate_dashboard(report_data)
        summary_file = generator.generate_weekly_summary(report_data)
        generated_files.extend(
            [
                ("Dashboard", dashboard_file),
                ("Weekly Summary", summary_file),
            ]
        )

    # Generate quality metrics for display
    generator.calculate_quality_metrics(report_data)

    if generated_files:
        for _name, _path in generated_files:
            pass


if __name__ == "__main__":
    main()
